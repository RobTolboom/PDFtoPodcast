# ğŸ“„ PDFtoPodcast: Medical Literature Extraction Pipeline

[![License: PPL 3.0.0](https://img.shields.io/badge/License-PPL%203.0.0-blue.svg)](LICENSE)
[![Commercial License Available](https://img.shields.io/badge/Commercial-License%20Available-green.svg)](COMMERCIAL_LICENSE.md)

**Intelligent medical literature data extraction pipeline using LLM vision capabilities for complete data fidelity.**

This pipeline extracts structured data from medical research PDFs with a focus on **preserving tables, figures, and complex formatting** that are critical for clinical trials and medical research papers.

---

## ğŸ”— Quick Links

| Documentation | Description |
|---------------|-------------|
| **[Installation](#-installation)** | Get started in 5 minutes |
| **[Usage Guide](#-usage)** | Web UI and CLI instructions |
| **[Architecture](#ï¸-architecture)** | Pipeline design and flow |
| **[Contributing](CONTRIBUTING.md)** | Development guidelines |
| **[API Reference](src/README.md)** | Module documentation |

---

## âœ¨ Key Features

- Direct PDF-to-LLM processing (no intermediate text extraction) to preserve tables, figures, and layout.
- Publication-type-aware schemas for interventional, observational, synthesis, prognosis, opinion, and other papers.
- Iterative validation/correction loop with configurable accuracy, completeness, and schema thresholds.
- Dual entry points: Streamlit dashboard for guided runs and CLI module for automation and scripting.
- Structured JSON outputs with deterministic file naming in `tmp/` for each pipeline step.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PDF Input (â‰¤100 pages, â‰¤32 MB)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Classification                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Upload PDF to LLM (vision)                           â”‚ â”‚
â”‚  â”‚ â€¢ Identify publication type (6 categories)             â”‚ â”‚
â”‚  â”‚ â€¢ Extract metadata (DOI, authors, journal, etc)        â”‚ â”‚
â”‚  â”‚ Output: classification.json                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Extraction                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ â€¢ Upload PDF to LLM (vision)                           â”‚ â”‚
â”‚  â”‚ â€¢ Type-specific schema (interventional/observational)  â”‚ â”‚
â”‚  â”‚ â€¢ Extract tables, figures, complete data               â”‚ â”‚
â”‚  â”‚ Output: extraction.json                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Validation & Correction (Iterative Loop)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ While quality insufficient AND iterations < max:       â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  3a. Validation (Dual-Tier)                            â”‚ â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚      â”‚ Schema validation (fast, free)                â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Check structure & types                     â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Quality score â‰¥50% â†’ proceed to LLM         â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Quality score <50% â†’ fail immediately       â”‚ â”‚ â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚      â”‚ LLM validation (if schema passed)             â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Upload PDF for comparison                   â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Semantic accuracy check                     â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ â€¢ Completeness verification                   â”‚ â”‚ â”‚
â”‚  â”‚      â”‚ Output: validation{N}.json                    â”‚ â”‚ â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  3b. Quality Assessment                                â”‚ â”‚
â”‚  â”‚      â€¢ Check completeness â‰¥90%                         â”‚ â”‚
â”‚  â”‚      â€¢ Check accuracy â‰¥95%                             â”‚ â”‚
â”‚  â”‚      â€¢ Check schema compliance â‰¥95%                    â”‚ â”‚
â”‚  â”‚      â€¢ Check critical_issues = 0                       â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  3c. If quality insufficient:                          â”‚ â”‚
â”‚  â”‚      â€¢ Run correction with validation feedback         â”‚ â”‚
â”‚  â”‚      â€¢ Upload PDF + validation report to LLM           â”‚ â”‚
â”‚  â”‚      â€¢ Fix identified issues                           â”‚ â”‚
â”‚  â”‚      â€¢ Output: extraction{N}.json                      â”‚ â”‚
â”‚  â”‚      â€¢ Loop back to 3a (validate corrected)            â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  3d. Early Stopping:                                   â”‚ â”‚
â”‚  â”‚      â€¢ If quality degrades 2 consecutive iterations    â”‚ â”‚
â”‚  â”‚      â€¢ Select best iteration and exit                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â”‚  Output: Best extraction + validation from all iterations    â”‚
â”‚  Status: passed / max_iterations_reached / early_stopped     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**For detailed architecture and design decisions, see [ARCHITECTURE.md](ARCHITECTURE.md)**

> The diagram shows provider hard limits (32 MB, 100 pages). By default the pipeline caps uploads at 10 MB
> to match the Streamlit uploader and `MAX_PDF_SIZE_MB` settingâ€”raise it in your `.env` only if your provider
> account allows larger PDFs.

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.10+ with `pip` and (optionally) `make` available on your PATH.
- Access to an OpenAI or Anthropic account with document/vision models enabled and a valid API key.
- Internet connectivity for LLM API calls and PDF uploads (approx. 1â€“3K tokens per PDF page).
- Local environment capable of opening a browser window for Streamlit (or a remote session that supports it).
- De-identify PDFs before processing; documents are transmitted to external LLM providers.

### Setup

```bash
# Clone repository
git clone https://github.com/RobTolboom/PDFtoPodcast.git
cd PDFtoPodcast

# (Optional) Create an isolated virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .\.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

Create a `.env` file in the project root (there is no committed template) and add your provider
credentials using the sample below.

Alternatively, run `make install` (or `make install-dev` for tooling) to reuse the Makefile targets.

### Environment Variables

Create `.env` file with:

```bash
# Required: Choose one or both providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# Optional: Model selection
OPENAI_MODEL=gpt-5           # Default: gpt-5 (vision support)
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # Default: Sonnet 4.5

# Optional: Token limits
OPENAI_MAX_TOKENS=128000     # Default in config; lower if your account has tighter limits
ANTHROPIC_MAX_TOKENS=4096

# Optional: Temperature (0.0 = deterministic)
LLM_TEMPERATURE=0.0

# Optional: Timeout (seconds)
LLM_TIMEOUT=600              # Config default, rounded up to allow long extractions

# Optional: PDF limits (API constraints)
MAX_PDF_PAGES=100             # Default: 100 (API limit)
MAX_PDF_SIZE_MB=10            # Pipeline default (increase up to 32 MB if your provider allows it)
```

You can omit any optional keysâ€”those shown above match the defaults baked into
`src/config.py`. Increase `MAX_PDF_SIZE_MB` only if your LLM provider account allows
larger uploads (OpenAI and Claude currently cap at 32 MB).

#### Obtaining API keys
- OpenAI: create a key from the [API Keys dashboard](https://platform.openai.com/account/api-keys).
- Anthropic: generate a key via the [Claude console](https://console.anthropic.com/settings/keys).
- Store keys securely (e.g., password manager) and avoid committing `.env` to version control.

---

## ğŸš€ Usage

### Web Interface (Recommended)

The easiest way to use PDFtoPodcast is through the **Streamlit web interface**:

```bash
streamlit run app.py
```

**Features:**
- ğŸ“¤ **Drag-and-drop PDF upload** with duplicate detection
- âš™ï¸ **Interactive pipeline configuration** (select steps, LLM provider, page limits)
- ğŸš€ **Real-time execution screen** with live progress tracking per step
- ğŸ‘ï¸ **Inspect existing results** from the Settings screen with JSON syntax highlighting
- ğŸ”„ **Re-run specific phases** by deselecting completed steps in Settings and starting the run again
- ğŸ“ **Previously uploaded files library** for easy file selection
- âš ï¸ **Intelligent error handling** with actionable recovery guidance
- ğŸ” **Verbose logging toggle** for detailed pipeline diagnostics

> â„¹ï¸ The dedicated â€œResultsâ€ dashboard is still under developmentâ€”after a run completes
> you return to Settings, where you can open or delete the generated JSON files.

**Perfect for:**
- First-time users
- Testing and experimentation
- Reviewing results interactively
- Real-time progress monitoring
- Non-technical users

---

### Command-Line Interface

For automation and batch processing, use the CLI:

```bash
# Run full pipeline (all 3 steps)
python run_pipeline.py path/to/paper.pdf

# Run with Claude
python run_pipeline.py path/to/paper.pdf --llm-provider claude

# Process only first 10 pages (for testing)
python run_pipeline.py path/to/paper.pdf --max-pages 10

# Run single step only
python run_pipeline.py path/to/paper.pdf --step classification

# Run iterative validation-correction with custom settings
python run_pipeline.py path/to/paper.pdf --step validation_correction \
    --max-iterations 2 \
    --completeness-threshold 0.85 \
    --accuracy-threshold 0.90

# Keep intermediate files
python run_pipeline.py path/to/paper.pdf --keep-tmp
```

#### Logs & troubleshooting
- CLI runs stream to the terminal via Richâ€”rerun with `--keep-tmp` to inspect intermediate JSON under `tmp/`.
- In Streamlit, enable **Verbose logging** in Settings to display per-step token usage and metadata in the Execution screen.
- Errors always include a concise remediation checklist; review the linked JSON artefacts for deeper context.

### Command-Line Options

```
usage: run_pipeline.py [-h] [--max-pages MAX_PAGES] [--keep-tmp]
                       [--llm-provider {openai,claude}]
                       [--step {classification,extraction,validation,correction,validation_correction}]
                       [--max-iterations MAX_ITERATIONS]
                       [--completeness-threshold FLOAT]
                       [--accuracy-threshold FLOAT]
                       [--schema-threshold FLOAT]
                       pdf

positional arguments:
  pdf                   Path to PDF file

optional arguments:
  -h, --help            Show help message
  --max-pages MAX_PAGES Limit number of pages (for testing)
  --keep-tmp            Keep intermediate files in tmp/
  --llm-provider {openai,claude}
                        Choose LLM provider (default: openai)
  --step {classification,extraction,validation,correction,validation_correction}
                        Run specific pipeline step (default: run all steps)
  --max-iterations MAX_ITERATIONS
                        Maximum correction attempts for validation_correction (default: 3)
  --completeness-threshold FLOAT
                        Minimum completeness score 0.0-0.99 (default: 0.90)
  --accuracy-threshold FLOAT
                        Minimum accuracy score 0.0-0.99 (default: 0.95)
  --schema-threshold FLOAT
                        Minimum schema compliance score 0.0-0.99 (default: 0.95)
```

### Programmatic Usage

```python
from pathlib import Path
from src.pipeline import run_four_step_pipeline

# Run complete pipeline
results = run_four_step_pipeline(
    pdf_path=Path("paper.pdf"),
    max_pages=20,  # Optional: limit pages
    llm_provider="openai",  # or "claude"
    breakpoint_after_step=None  # Optional: "classification", "extraction", etc.
)

# Access results
print(f"Type: {results['classification']['publication_type']}")
print(f"Extracted fields: {len(results['extraction'])}")
```

**For detailed API documentation, see [src/README.md](src/README.md)**

---

## ğŸ“Š Supported Publication Types

The pipeline supports 6 publication type categories:

| Type | Description | Examples |
|------|-------------|----------|
| `interventional_trial` | Clinical trials with intervention | RCT, Cluster-RCT, Before-after, Single-arm |
| `observational_analytic` | Observational studies | Cohort, Case-control, Cross-sectional |
| `evidence_synthesis` | Systematic reviews | Meta-analysis, Network meta-analysis, Systematic review |
| `prediction_prognosis` | Prediction models | Risk prediction, Prognostic models, ML algorithms |
| `editorials_opinion` | Opinion pieces | Editorial, Commentary, Letter to editor |
| `overig` | Other | Case reports, Narrative reviews, Guidelines |

Each type has a specialized extraction schema optimized for its data structure.

---

## ğŸ“‚ Output Structure

All outputs are saved in `tmp/` directory with PDF filename-based naming:

```
tmp/
â”œâ”€â”€ sample_paper-classification.json
â”œâ”€â”€ sample_paper-extraction0.json
â”œâ”€â”€ sample_paper-validation0.json
â”œâ”€â”€ sample_paper-extraction1.json          # Iteration 1 correction (if triggered)
â”œâ”€â”€ sample_paper-validation1.json          # Validation after correction 1
â”œâ”€â”€ sample_paper-extraction-best.json      # Best-scoring extraction (any iteration)
â”œâ”€â”€ sample_paper-validation-best.json      # Validation paired with best extraction
â””â”€â”€ sample_paper-extraction-best-metadata.json
```

Iteration files are numbered (`extraction0`, `extraction1`, â€¦) so you can track every correction pass. When the pipeline selects a best iteration it also writes `*-best.json` artefacts plus metadata about the choice. Failed runs may emit `*-failed.json` diagnostics.

### Output Format

Each JSON file contains structured data conforming to its schema:

**Classification output:**
```json
{
  "metadata": {
    "title": "Study Title",
    "doi": "10.1186/s12871-025-02345-6",
    "journal": "BMC Anesthesiology",
    "authors": [...]
  },
  "publication_type": "interventional_trial",
  "classification_confidence": 0.95
}
```

**Extraction output:**
```json
{
  "schema_version": "v2.0",
  "metadata": {...},
  "study_design": {...},
  "population": {...},
  "interventions": [...],
  "outcomes": [...],
  "results": {...}
}
```

---

## ğŸ”„ Iterative Validation-Correction

Selecting the `validation_correction` step (CLI or Streamlit) triggers an iterative loop that validates each extraction and, if needed, re-prompts the LLM with targeted feedback.

- Default thresholds (completeness â‰¥90%, accuracy â‰¥95%, schema â‰¥95%, critical issues = 0) are configurable via CLI flags or the Streamlit Settings sliders.
- The first pass always runs schema validation; LLM validation only executes when schema quality is at least 50%.
- Up to three correction attempts run after the initial extraction. The loop stops early if quality degrades, schema quality drops below 50%, or the provider repeatedly errors.
- Every iteration writes JSON artefacts to `tmp/` (e.g., `paper-extraction1.json`, `paper-validation1.json`). The pipeline returns the best-scoring iteration.

Refer to [VALIDATION_STRATEGY.md](VALIDATION_STRATEGY.md) for scoring formulas, prompts, and trade-offs.

---

## ğŸ’° Cost Considerations

### PDF Upload vs Text Extraction

| Approach | Tokens/Page | Data Quality | Best For |
|----------|-------------|--------------|----------|
| **Text extraction** (old) | ~500 | âŒ Tables lost | Simple documents |
| **PDF upload** (current) | ~1,500-3,000 | âœ… Complete fidelity | Medical research |

### Cost Example (20-page paper)

| Provider | Input Cost | Output Cost | Total per Paper |
|----------|------------|-------------|-----------------|
| **OpenAI GPT-5** | $0.20 (40k tokens) | $0.60 (4k tokens) | **~$0.80** |
| **Claude Opus 4.1** | $0.60 (40k tokens) | $2.40 (4k tokens) | **~$3.00** |

**Full pipeline (4 steps):** ~$3-12 per paper depending on provider and corrections needed.

> Pricing snapshot: October 2024 public rate cards. Recalculate with your providerâ€™s latest pricing and negotiated discounts.

---

## ğŸ¯ Validation Strategy (Summary)

- Tier 1: Local schema validation with `jsonschema` verifies structure and required fields in milliseconds.
- Tier 2: Optional LLM validation cross-checks content against the PDF when schema quality meets the `SCHEMA_QUALITY_THRESHOLD` (default 0.5).
- Thresholds, retries, and correction prompts are documented in [VALIDATION_STRATEGY.md](VALIDATION_STRATEGY.md). Start there before altering validation behaviour.

---

## ğŸ› ï¸ Development

### Project Structure

```
PDFtoPodcast/
â”œâ”€â”€ run_pipeline.py              # Main CLI entry point
â”œâ”€â”€ app.py                       # Streamlit web UI entry point
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ .env                         # Configuration (create manually; see README)
â”œâ”€â”€ src/                         # Core modules
â”‚   â”œâ”€â”€ config.py                # Settings & configuration
â”‚   â”œâ”€â”€ prompts.py               # Prompt loading
â”‚   â”œâ”€â”€ schemas_loader.py        # Schema management
â”‚   â”œâ”€â”€ validation.py            # Validation utilities
â”‚   â”œâ”€â”€ llm/                     # LLM provider package
â”‚   â”œâ”€â”€ pipeline/                # Pipeline orchestration
â”‚   â””â”€â”€ streamlit_app/           # Web UI components
â”œâ”€â”€ prompts/                     # Prompt templates
â”œâ”€â”€ schemas/                     # JSON schemas
â”œâ”€â”€ tests/                       # Test suite
â””â”€â”€ tmp/                         # Output directory (auto-created)
```

**For complete module documentation, see [src/README.md](src/README.md)**

### Quick Development Commands

```bash
# Development workflow
make format       # Format code
make lint         # Check for issues
make typecheck    # Type checking
make check        # All checks

# Testing
make test         # Run all tests
make test-fast    # Quick tests
make test-coverage # With coverage

# Git workflow
make commit       # Prepare for commit
git commit -m "type: description"
git push
```

**For complete development guidelines, see [CONTRIBUTING.md](CONTRIBUTING.md)**

---

## ğŸ”’ API Limits & Constraints

### OpenAI API Limits
- **Provider limit:** 100 pages, 32 MB per PDF
- **Pipeline default upload size:** 10 MB (adjust with `MAX_PDF_SIZE_MB`)
- **Models:** gpt-5 (vision-capable model)
- **Format:** Base64-encoded PDF

### Claude API Limits
- **Provider limit:** 100 pages, 32 MB per PDF
- **Pipeline default upload size:** 10 MB (adjust with `MAX_PDF_SIZE_MB`)
- **Models:** Claude Opus 4.1, Sonnet 4.5, Haiku 3.5
- **Format:** Base64-encoded PDF with `application/pdf` media type

---

## ğŸ“š Documentation

| Document | Purpose |
|----------|---------|
| **[README.md](README.md)** (this file) | Quick start and overview |
| **[src/README.md](src/README.md)** | Module API reference |
| **[ARCHITECTURE.md](ARCHITECTURE.md)** | System design and decisions |
| **[CONTRIBUTING.md](CONTRIBUTING.md)** | Development guidelines |
| **[DEVELOPMENT.md](DEVELOPMENT.md)** | Development workflow |
| **[VALIDATION_STRATEGY.md](VALIDATION_STRATEGY.md)** | Dual validation approach |
| **[prompts/README.md](prompts/README.md)** | Prompt engineering |
| **[schemas/readme.md](schemas/readme.md)** | Schema design |
| **[CHANGELOG.md](CHANGELOG.md)** | Version history |

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines
2. Fork the repository
3. Create a feature branch
4. Add tests for new features
5. Update documentation
6. Submit pull request

---

## ğŸ“ License

This project uses a **dual-license model**:

### Free Use (Prosperity Public License 3.0.0)
- âœ… **Academic research** - Free forever
- âœ… **Non-commercial use** - Free forever
- âœ… **Commercial trial** - Free for 30 days (company-wide)
- ğŸ“„ See [LICENSE](LICENSE) for full terms

### Commercial Use
- ğŸ’¼ After 30-day trial, commercial use requires a paid license
- ğŸ“Š Flexible pricing: Subscription or Pay-per-PDF
- ğŸ¢ Self-hosted on your infrastructure
- ğŸ“„ See [COMMERCIAL_LICENSE.md](COMMERCIAL_LICENSE.md) for terms
- ğŸ“§ For commercial licensing inquiries: Open a GitHub issue or discussion

---

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue.

---

**Built with â¤ï¸ for medical research data extraction**
